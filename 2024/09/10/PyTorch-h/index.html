<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>PyTorch_h | Welcome</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Tensor &amp; autogradTensor支持gpu加速，可以在ipython&#x2F;notebook使用&lt;function&gt;? 查看帮助文档,也可以在代码中用help(function)这里的function就不用加小括号了，如 12torch.ones? #仅用于notebook&#x2F;ipython#help(torch.ones)    基础操作从接口，对tensor">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch_h">
<meta property="og:url" content="https://tjzhang-src.github.io/.github.io/2024/09/10/PyTorch-h/index.html">
<meta property="og:site_name" content="Welcome">
<meta property="og:description" content="Tensor &amp; autogradTensor支持gpu加速，可以在ipython&#x2F;notebook使用&lt;function&gt;? 查看帮助文档,也可以在代码中用help(function)这里的function就不用加小括号了，如 12torch.ones? #仅用于notebook&#x2F;ipython#help(torch.ones)    基础操作从接口，对tensor">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2024-09-10T01:53:50.000Z">
<meta property="article:modified_time" content="2024-09-10T01:54:13.331Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/.github.io/img/favicon.png"><link rel="canonical" href="https://tjzhang-src.github.io/.github.io/2024/09/10/PyTorch-h/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="stylesheet" href="/.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4cc6526274b17acfe67a4be7d70e8d1c";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-FF1BJEGK51"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-FF1BJEGK51');
</script><script>const GLOBAL_CONFIG = { 
  root: '/.github.io/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'PyTorch_h',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-10 09:54:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/.github.io/archives/"><div class="headline">文章</div><div class="length-num">46</div></a><a href="/.github.io/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/.github.io/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/.github.io/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/.github.io/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/.github.io/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/.github.io/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/02/26/9fQnztK2ImqyLwS.webp')"><nav id="nav"><span id="blog-info"><a href="/.github.io/" title="Welcome"><span class="site-name">Welcome</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/.github.io/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/.github.io/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/.github.io/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/.github.io/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch_h</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-10T01:53:50.000Z" title="发表于 2024-09-10 09:53:50">2024-09-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-10T01:54:13.331Z" title="更新于 2024-09-10 09:54:13">2024-09-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="PyTorch_h"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Tensor-amp-autograd"><a href="#Tensor-amp-autograd" class="headerlink" title="Tensor &amp; autograd"></a>Tensor &amp; autograd</h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>支持gpu加速，可以在ipython&#x2F;notebook使用&lt;function&gt;? 查看帮助文档,也可以在代码中用help(function)这里的function就不用加小括号了，如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.ones? #仅用于notebook/ipython</span><br><span class="line">#help(torch.ones)  </span><br></pre></td></tr></table></figure>

<h3 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h3><p>从接口，对tensor操作分为两类（torch.sum(a, b), a.sum(b)）：</p>
<ul>
<li>torch.function, 如torch.save (torch 是导入的模块名)</li>
<li>tensor.function, 如tensor.view (tensor代表的是创建的tensor对象名，)</li>
</ul>
<p>从存储，对tensor操作分为两类：</p>
<ul>
<li>不会修改自身数据，如a.add(b),会返回一个新的tensor</li>
<li>会修改自身数据，如a.add_(b)</li>
</ul>
<h4 id="创建tensor方式："><a href="#创建tensor方式：" class="headerlink" title="创建tensor方式："></a>创建tensor方式：</h4><img src="/.github.io/2024/09/10/PyTorch-h/image-20231122102327188.png" class title="image-20231122102327188">

<p>Tensor函数是最复杂多变的方式，可以接收list，可以指定形状，可以传入其他的tensor，或者torch.Size类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">b = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">c = t.Tensor(a)</span><br><span class="line">b.tolist() k<span class="comment">#可以把tensor转会list</span></span><br></pre></td></tr></table></figure>

<p>tensor.size() 返回torch.Size 对象，可查看tensor大小，也可以作为Tensor创建函数的参数。tensor.shape等同于tensor.size()</p>
<h4 id="常用tensor操作"><a href="#常用tensor操作" class="headerlink" title="常用tensor操作"></a>常用tensor操作</h4><h5 id="tensor-view"><a href="#tensor-view" class="headerlink" title="tensor.view"></a>tensor.view</h5><p>改变tensor的形状，返回的tensor与原tensor共享内存（相当于只是变了映射关系）</p>
<p>torch.squeeze(a, [N]),代表去除a中维度为一的维度，不输入N代表去除所有维度为一的维度，为具体数值表示如果该维度为1，则将其去掉。</p>
<p>torch.unsqueeze(a, [N])对数据进行扩充，在指定位置加上维度为一的维度</p>
<p>resize是另一种调整size的办法，但如果新尺寸超过了原尺寸，会分配新的空间，如果新尺寸小于了原尺寸，之前的数据会被保留</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = t.randn(2, 3)</span><br><span class="line">a.resize_(3, 4)   #貌似没见到a.resize()的用法，帮助文档也找不到，</span><br><span class="line">print(a.size(), a)</span><br><span class="line">a.resize_?</span><br><span class="line">--------------------------------------------------</span><br><span class="line">torch.Size([3, 4]) tensor([[ 1.1888e+00, -2.2413e-01, -6.4524e-01,  1.1506e+00],</span><br><span class="line">        [-6.9476e-02, -1.4372e+00,  1.8057e+28,  7.9050e+31],</span><br><span class="line">        [ 1.3236e-14,  3.9890e+03,  1.7033e+19,  1.0903e+27]])</span><br></pre></td></tr></table></figure>

<h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><p>无特殊说明，索引出来的结果与原tensor共享内存，</p>
<p>一般来说可以这么用 <code>a[1:2, 4:6, 1:3 ...]</code>以逗号分割的代表不同维度，可以小于tensor的维度，以冒号分割的代表该维度上下限，左开又闭。</p>
<p>注意后两者区别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(a.size())</span><br><span class="line">print(a[0:1, :2].size())</span><br><span class="line">print(a[0, :2].size())</span><br><span class="line">-----------------------------------</span><br><span class="line">torch.Size([3, 4])</span><br><span class="line">torch.Size([1, 2])</span><br><span class="line">torch.Size([2])</span><br></pre></td></tr></table></figure>

<p>一个tensor，可以用 a &gt; 1返回一个bytetensor,也可以作为tensor的索引，等同于a.mask_select(a&gt;1)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = t.randn(3, 4)</span><br><span class="line">print(a)</span><br><span class="line">print(a[a&gt;1])</span><br><span class="line">------------------------------</span><br><span class="line">tensor([[ 1.3819,  1.4626, -0.0858, -1.2977],</span><br><span class="line">        [-0.1992,  0.1883,  0.5572, -2.4197],</span><br><span class="line">        [ 2.4179, -0.0833,  0.6245, -0.6284]])</span><br><span class="line">tensor([1.3819, 1.4626, 2.4179])</span><br></pre></td></tr></table></figure>

<p>其他的选择函数：</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122111914158.png" class title="image-20231122111914158">

<p>对一个二维gather，输出的每个元素如下</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122113254206.png" class title="image-20231122113254206">

<p>与gather对应的逆操作是scatter_。</p>
<h5 id="高级索引"><a href="#高级索引" class="headerlink" title="高级索引"></a>高级索引</h5><p>可以看成普通索引的拓展，但是结果一般不和原始Tensor共享内存。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = t.arange(0, 27).view(3, 3, 3)</span><br><span class="line">print(x)</span><br><span class="line">print(x[[1, 2],[1, 2], [2, 0]])</span><br><span class="line">-------------------------------------</span><br><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 3,  4,  5],</span><br><span class="line">         [ 6,  7,  8]],</span><br><span class="line"></span><br><span class="line">        [[ 9, 10, 11],</span><br><span class="line">         [12, 13, 14],</span><br><span class="line">         [15, 16, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 19, 20],</span><br><span class="line">         [21, 22, 23],</span><br><span class="line">         [24, 25, 26]]])</span><br><span class="line">tensor([14, 24])</span><br></pre></td></tr></table></figure>

<p>这与之前带 冒号的索引很相似，逗号分割的是不同维度，可以小于tensor的维度，只不过原来是i:j ，现在是[a, b],表示a或b，不同维度之间的匹配次数不愿意，原来是i*j，现在是各维度的元素数量（应该，有点不好描述）</p>
<h5 id="Tensor类型"><a href="#Tensor类型" class="headerlink" title="Tensor类型"></a>Tensor类型</h5><p>tensor有不同类型，每种类型分别对应有CPU和GPU版本的，默认tensor是floattensor类型，可以通过t.set_default_tensor_type 修改tensor类型，（如果默认类型为gpu tensor，那么所有操作都将在GPU上进行）,</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122145243071.png" class title="image-20231122145243071">

<p>各类型之间可以进行转换，**type(new_type)**是通用的做法，除此之外，还有float，long，half等快捷方式。GPU tensor 和 CPU tensor之间的相互转换可以通过 tensor.cuda 和 tensor.cpu的方法实现，Tensor还有一个new 方法，可以创建该tensor类型的tensor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = t.Tensor(2, 3)</span><br><span class="line">b = a.long()</span><br><span class="line">c = a.type_as(b)</span><br><span class="line">d = c.new(3, 4)</span><br></pre></td></tr></table></figure>

<h5 id="逐元素操作"><a href="#逐元素操作" class="headerlink" title="逐元素操作"></a>逐元素操作</h5><p>element-wise，操作输入与输出形状一致。</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122151449753.png" class title="image-20231122151449753">

<h5 id="归并操作"><a href="#归并操作" class="headerlink" title="归并操作"></a>归并操作</h5><p>使输出形状小于输入形状，并可以沿着某一维度指定操作。如加法sum，可以计算整个tensor的和，也可以计算每一行&#x2F;每一列的和。</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122152116695.png" class title="image-20231122152116695">

<p>以上函数都有一个参数dim，用来指定是在那个维度上操作的，另外一个注意点：（wuenda视频里也经常有keepdims&#x3D;True）</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122154306601.png" class title="image-20231122154306601">

<h5 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h5><p>有些是逐元素比较，有些类似于归并操作。常用函数如下：</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122160645303.png" class title="image-20231122160645303">

<p>第一行实现了运算符重载可以用<code>a&gt;=b, a!=b </code>等，返回结果是一个bytetensor。max&#x2F;min有三种用法，</p>
<ul>
<li>t.max(tensor) 返回tensor中最大的一个数</li>
<li>t.max(tensor, dim) <strong>指定维度</strong>上最大的数，返回tensor和下标（如二维tenser，dim&#x3D;1，代表每一行的最大值）</li>
<li>t.max(tensor, tensor) 比较两个tensor比较大的元素</li>
</ul>
<blockquote>
<p>维度，整个多维矩阵可以看出一棵树，从第一维到第n维，max的第二种用法需要指定dim为第几维度，对应到树上就是第几层树，然后在该层间找最大值（同一个父节点）</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231122183635877.png" class title="image-20231122183635877">

<p>例子：</p>
<p>x &#x3D; numpy.array([[[1, 2], [3, 4]],       [[4, 3], [2, 1]]])  （pytorch也一样）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">print(numpy.max(x, axis=0))</span><br><span class="line">print(all(numpy.max(x, axis=0)==numpy.max(x, axis=-3)))</span><br><span class="line">----------------------------------</span><br><span class="line">[[4, 3],</span><br><span class="line">[3, 4]]</span><br><span class="line">True</span><br><span class="line">print(numpy.max(x, axis=1))</span><br><span class="line">print(all(numpy.max(x, axis=1)==numpy.max(x, axis=-2)))</span><br><span class="line">-----------------------------------</span><br><span class="line">[[3, 4],</span><br><span class="line">[4, 3]]</span><br><span class="line">True</span><br><span class="line">print(numpy.max(x, axis=2))</span><br><span class="line">print(all(numpy.max(x, axis=2)==numpy.max(x, axis=-1)))</span><br><span class="line">------------------------------------</span><br><span class="line">[[2, 4],</span><br><span class="line">[4, 2]]</span><br><span class="line">True</span><br></pre></td></tr></table></figure>


</blockquote>
<h5 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h5><p>pytorch的线性函数主要封装了blas和lapack，常用的函数如下表所示</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231123104238447.png" class title="image-20231123104238447">



<h3 id="Tensor-amp-Numpy"><a href="#Tensor-amp-Numpy" class="headerlink" title="Tensor &amp; Numpy"></a>Tensor &amp; Numpy</h3><p>两者相似性很高，之间共享内存。可以相互转换再使用。</p>
<p>虽然pytorch有自动广播，还是可以通过以下函数手动实现，不易出错</p>
<ul>
<li>unsqueeze或view，为某一维度补上1</li>
<li>expand或expand_as, 重复数组，将单维度扩大。（不会复制数组，所有不会占用额外空间，repeat有类似的功能，但会复制数据，会占用额外空间）</li>
</ul>
<h3 id="内部结构"><a href="#内部结构" class="headerlink" title="内部结构"></a>内部结构</h3><p>tensor分为<strong>头信息区和存储区</strong>。信息区主要保存着tensor形状，步长，数据类型等信息。真正的数据连续成组，信息区占用内存较少，主要内存占用取决于tensor中元素的数目，即存储区的大小。</p>
<p>可以通过对象的storage函数访问，将其传入id中获取对象id, 可以通过对象的data_ptr 返回tensor的首元素的内存地址。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(b.storage())</span><br><span class="line">print(id(b.storage()))</span><br></pre></td></tr></table></figure>

<h3 id="other"><a href="#other" class="headerlink" title="other"></a>other</h3><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>tensor加载和保存很简单，使用t.save和t.load,在save&#x2F;load时还可以指定使用到pickle模块，load时可将gpu tensor映射到其他cpu或gpu上。（不太理解，之后看吧）</p>
<h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p>避免原生for循环，使用向量化的数值计算。</p>
<blockquote>
<p>疑问，pytorch 的tensor变量在被赋值时可以自动更改类型吗</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个整数类型的张量</span><br><span class="line">a = t.tensor([1, 2, 3])</span><br><span class="line"></span><br><span class="line"># 尝试将浮点数赋值给整数类型的张量</span><br><span class="line">a = t.tensor([1.0, 2.0, 3.0])</span><br><span class="line"></span><br><span class="line"># 查看最终张量的类型</span><br><span class="line">print(a.dtype)</span><br></pre></td></tr></table></figure>


</blockquote>
<h3 id="小试牛刀-线性回归"><a href="#小试牛刀-线性回归" class="headerlink" title="小试牛刀-线性回归"></a>小试牛刀-线性回归</h3><p><code>%matplotlib inline</code> 将使得 matplotlib 的图表直接嵌入到 notebook 中的输出单元格中，而不是弹出一个新的图形窗口。这对于在 notebook 中进行数据分析和可视化非常方便。</p>
<h2 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h2><p>方便用户使用，专门开发的自动求导引擎，根据输入和前向传播过程自动构建计算图，执行反向传播。了解基本计算图知识很有必要（Christopher Olah）</p>
<h3 id="Variable（部分可能是老版本的）"><a href="#Variable（部分可能是老版本的）" class="headerlink" title="Variable（部分可能是老版本的）"></a>Variable（部分可能是老版本的）</h3><p>Pytorch在autograd模块中实现了计算图的相关功能，autograd中的核心计算结构是Variable，Variable封装了tensor。Variable包含三个属性，data(保存variable包含的tensor)，grad（保存data对应的梯度），grad_fn（指向一个function，记录variable的操作历史）,</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231124160752724.png" class title="image-20231124160752724">

<p>Variable的构造参数需要传入tensor，同时有两个可选参数。</p>
<p>requires_grad(bool) ，是否需要对该variable进行求导。</p>
<p>vaolatile(bool) ,设置为true，构建在该variable上的图都不会求导。</p>
<p>Variable支持大部分tensor支持的函数，但不支持部分inplace函数，因为这些函数会修改tensor本身，在反向传播中，variable需要缓存之前的tensor来计算梯度。要计算各个Variable的梯度，只需要调用根节点variable的backward方法。autograd会自动沿着计算图反向传播，计算每一个叶子结点的梯度。</p>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>由于中间变量的梯度计算后会被清空，可以使用autograd.grad和hook来获取中间变量的梯度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.autograd.grad(z, y) #代表求z对y的梯度，隐式调用backward</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def variable_hook(grad):</span><br><span class="line">    print(&quot;y 的梯度&quot;, grad)</span><br><span class="line">hook_handle = y.register_hook(variable_hook) #注册,backward时可以自动调用</span><br><span class="line">hook_handle.remove()#去除</span><br></pre></td></tr></table></figure>

<p>假如<code>y = x**2 + x*2, z = y.sum()</code>可以从z开始backward，也可以从y开始，只是需要传入y当前的梯度，之所以z不用传，是因为z对自己的梯度就是1，被省略了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = t.arange(0.0, 3.0, requires_grad=True)</span><br><span class="line">y = x**2 + x * 2</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line">x.grad</span><br><span class="line">--------------------------------------------------</span><br><span class="line">x = t.arange(0.0, 3.0, requires_grad=True)</span><br><span class="line">y = x**2 + x * 2</span><br><span class="line">z = y.sum()</span><br><span class="line">y_grad_var = t.Tensor([1.0, 1.0, 1.0])</span><br><span class="line">y.backward(y_grad_var)</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>

<p>对variable的操作才可以使用autograd，如果对<strong>variable的data</strong>直接操作，将无法使用反向传播。一般不会修改data的值。</p>
<h1 id="神经网络工具箱nn"><a href="#神经网络工具箱nn" class="headerlink" title="神经网络工具箱nn"></a>神经网络工具箱nn</h1><p>针对前向传播网络，每次都写很复杂的forward函数含麻烦，有两种简化方式，ModuleList和Sequential。Sequential是一个特殊的Module，它包含几个module，前向传播会将输入一层一层的传递下去，Module也是一个特殊的Module，可以包含几个module，向使用list一样使用。</p>
<p>两种创建</p>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>常用优化方法封装到torch.optim中，所有优化方法都是继承基类optim.Optimizer，</p>
<p>nn.module 和nn.fucntional, 模型有可学习的参数，最好用第一个，否则都可。激活函数（relu，sigmoid，tanh，）池化没有可学习的参数，可以用对应的functional函数代替。卷积，全连接等网络建议用nn.module。有参数也可以用funtional，只是需要手动定义参数，如。</p>
<img src="/.github.io/2024/09/10/PyTorch-h/image-20231205104540178.png" class title="image-20231205104540178">



<h1 id="常用工具"><a href="#常用工具" class="headerlink" title="常用工具"></a>常用工具</h1><p>数据，可视化，GPU加速</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>数据加载，通过自定义数据集实现，数据集对象被抽象为dataset类，实现自定义的数据集需要继承dataset，并实现两个魔法方法。</p>
<ul>
<li>__getitem__， 返回一个数据</li>
<li><strong>len</strong> ，返回样本的数量</li>
</ul>
<p>此外，pytorch提供了torchvision，是一个视觉工具包，提供了很多视觉图像处理的工具。transforms模块提供了对PIL Image对象和Tensor对象的常用操作。</p>
<p>对PIL iMAGE的常用操作：</p>
<ul>
<li>Resize </li>
<li>centerCrop</li>
<li>pad</li>
<li>toTensor,并且自动归一化[0,1]</li>
<li>normalize</li>
<li>topilimage</li>
</ul>
<p>torchvision 视觉工具包，常用模型，数据集加载方式，预处理操作</p>
<p>可视化工具，Tensorboard和visdom</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://tjzhang-src.github.io/.github.io">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://tjzhang-src.github.io/.github.io/2024/09/10/PyTorch-h/">https://tjzhang-src.github.io/.github.io/2024/09/10/PyTorch-h/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://tjzhang-src.github.io/.github.io" target="_blank">Welcome</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/.github.io/2024/09/17/cplusplusprimer/" title="cplusplusprimer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">cplusplusprimer</div></div></a></div><div class="next-post pull-right"><a href="/.github.io/2023/04/13/%E7%BB%8F%E9%AA%8C%E8%AE%B0%E5%BD%95/" title="经验记录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">经验记录</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Waline</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/.github.io/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">John Doe</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/.github.io/archives/"><div class="headline">文章</div><div class="length-num">46</div></a><a href="/.github.io/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/.github.io/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/tjzhang-src"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensor-amp-autograd"><span class="toc-number">1.</span> <span class="toc-text">Tensor &amp; autograd</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor"><span class="toc-number">1.1.</span> <span class="toc-text">Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.1.</span> <span class="toc-text">基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtensor%E6%96%B9%E5%BC%8F%EF%BC%9A"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">创建tensor方式：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8tensor%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">常用tensor操作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#tensor-view"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">tensor.view</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text">索引操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E7%B4%A2%E5%BC%95"><span class="toc-number">1.1.1.2.3.</span> <span class="toc-text">高级索引</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tensor%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.1.1.2.4.</span> <span class="toc-text">Tensor类型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%80%90%E5%85%83%E7%B4%A0%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.1.2.5.</span> <span class="toc-text">逐元素操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BD%92%E5%B9%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.1.2.6.</span> <span class="toc-text">归并操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AF%94%E8%BE%83"><span class="toc-number">1.1.1.2.7.</span> <span class="toc-text">比较</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">1.1.1.2.8.</span> <span class="toc-text">线性代数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor-amp-Numpy"><span class="toc-number">1.1.2.</span> <span class="toc-text">Tensor &amp; Numpy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.3.</span> <span class="toc-text">内部结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#other"><span class="toc-number">1.1.4.</span> <span class="toc-text">other</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">持久化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">向量化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E8%AF%95%E7%89%9B%E5%88%80-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.5.</span> <span class="toc-text">小试牛刀-线性回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#autograd"><span class="toc-number">1.2.</span> <span class="toc-text">autograd</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Variable%EF%BC%88%E9%83%A8%E5%88%86%E5%8F%AF%E8%83%BD%E6%98%AF%E8%80%81%E7%89%88%E6%9C%AC%E7%9A%84%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">Variable（部分可能是老版本的）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.2.2.</span> <span class="toc-text">计算图</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1nn"><span class="toc-number">2.</span> <span class="toc-text">神经网络工具箱nn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.1.</span> <span class="toc-text">优化器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7"><span class="toc-number">3.</span> <span class="toc-text">常用工具</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">3.1.</span> <span class="toc-text">数据</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/.github.io/2025/08/02/muduo-part2/" title="muduo_part2">muduo_part2</a><time datetime="2025-08-02T07:25:23.000Z" title="发表于 2025-08-02 15:25:23">2025-08-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/.github.io/2025/04/08/muduo/" title="muduo">muduo</a><time datetime="2025-04-08T01:30:10.000Z" title="发表于 2025-04-08 09:30:10">2025-04-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/.github.io/2025/02/22/xv6/" title="xv6">xv6</a><time datetime="2025-02-22T13:38:24.000Z" title="发表于 2025-02-22 21:38:24">2025-02-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/.github.io/2024/12/04/Linux%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%96%E7%A8%8B/" title="Linux高性能服务器编程">Linux高性能服务器编程</a><time datetime="2024-12-04T09:01:31.000Z" title="发表于 2024-12-04 17:01:31">2024-12-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/.github.io/2024/12/03/%E5%9B%BE%E8%A7%A3HTTP/" title="图解HTTP">图解HTTP</a><time datetime="2024-12-03T03:35:49.000Z" title="发表于 2024-12-03 11:35:49">2024-12-03</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/02/26/9fQnztK2ImqyLwS.webp')"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By John Doe</div><div class="footer_custom_text">Hi, welcome to my <a href="https://tjzhang-src.github.io/.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/.github.io/js/utils.js"></script><script src="/.github.io/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadWaline () {
  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://blog-comment-bfstu88s1-ttj2023.vercel.app/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: false,
    }, null))
  }

  const walineCSSLoad = document.getElementById('waline-css')

  if (typeof Waline === 'object') {
    walineCSSLoad ? initWaline() : getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css','waline-css').then(initWaline)
  }
  else {
    getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css','waline-css').then(() => {
      getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
    })
  }
}

if ('Waline' === 'Waline' || !true) {
  if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://tjzhang-src.github.io/.github.io/2024/09/10/PyTorch-h/'
    this.page.identifier = '/.github.io/2024/09/10/PyTorch-h/'
    this.page.title = 'PyTorch_h'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Waline' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>